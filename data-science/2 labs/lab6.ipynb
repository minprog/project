{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6: Scikit-Learn\n",
    "\n",
    "**Goal:** In this notebook we are going to start applying machine learning methods to train statistical models and explore new methods of inference and prediction. We will focus on Supervised Learning, both regression (learn continuous labels) and classification (learn discrete labels). In particular, we will introduce the Python library Scikit-learn and play with:\n",
    "- Linear and Polynomial regression\n",
    "- Naive Bayes\n",
    "\n",
    "The goals of this notebook are to test the application of Regression and Classification methods to structured and unstructured datasets; to apply model validation methods; and to perform feature engineering on unstructured text. \n",
    "\n",
    "The practical aspects of this Lab will be based on the **Chapter 5** (up to section *In Depth: Linear Regression*) of the [*Python Data Science Handbook*](https://jakevdp.github.io/PythonDataScienceHandbook/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Linear and polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will apply Scikit learn to create a Linear Regression model. Similar to other methods that use Estimator API of Scikit learn library, training and applying Linear Regression is a 5-Step process:\n",
    "\n",
    "1. Select model (Linear Regression, in this case) and import it\n",
    "2. Select model hyperparameters\n",
    "3. Arrange data in feature matrix (or vector if just 1 feature) and Target array\n",
    "4. Fit model to data\n",
    "5. Apply model to inference and prediction problems\n",
    "\n",
    "We will apply Linear Regression to better **understand 1) the relationship between GDP and Life Expectancy and 2) the relationship between Life expectancy and *happiness* in a specific year (2016)**.\n",
    "\n",
    "First, we will load and vizualize the required data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "happinessdataframe = pd.read_excel('happiness.xls', index_col=[0,1])\n",
    "\n",
    "x = happinessdataframe.dropna().loc(axis=0)[:,2016][[\"Log GDP per capita\",\"Healthy life expectancy at birth\"]]\n",
    "\n",
    "x.plot.scatter(x=\"Log GDP per capita\", y=\"Healthy life expectancy at birth\",c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you have an example of ordinary least squares Linear Regression.\n",
    "\n",
    "LinearRegression fits a linear model with coefficients $\\vec{w} = (w_0, w_1,..., w_p)$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "When $p=1$ (that is, a feature space with 1 dimension), LinearRegression fits a straight line to the data. Such line (as any other straight line) can be defined by two parameters: intercept ($w_0$) and slope ($w_1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Steps to use the Scikit-learn Estimator API:\n",
    "\n",
    "# 1. Select model and import it\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 2. Select model hyperparameters\n",
    "model = LinearRegression(fit_intercept=True) # select model hyperparameters\n",
    "\n",
    "# 3. Arrange data in feature matrix (or vector if just 1 feature) and Target array\n",
    "X = x[\"Log GDP per capita\"].values\n",
    "Y = x[\"Healthy life expectancy at birth\"].values\n",
    "\n",
    "# 4. Fit model to data\n",
    "model.fit(X=X[:, np.newaxis], y=Y) # / np.newaxis increases dimension of array by 1\n",
    "\n",
    "# 5. Apply model (in this case, visualize the regression line)\n",
    "xfit = np.linspace(min(X), max(X), num=2) # / linspace returns matrix with evenly spaced numbers\n",
    "yfit = model.predict(X=xfit[:, np.newaxis])\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, Y, c='black')\n",
    "plt.plot(xfit, yfit, c='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "What is the slope (or coefficient) and intercept (or bias) of the model created? What is the slope telling us about the relationship between GDP and life expectancy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Can you write down an expression for the straight red line in the previous plot, using the values of intercept_ and coef_ that were just printed? To confirm your solution, do a line plot where you overlap the red plot in the figure below (red) with another plot with the straight line equation you derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression can also be applied after expanding the feature space to include non-linear terms. This is convenient to capture non-linear relationships in data; notice that the number of coefficients will also increase. \n",
    "\n",
    "Let us now inspect the relationship between Life expectancy and Happiness. First we plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = happinessdataframe.dropna().loc(axis=0)[:,2016][[\"Healthy life expectancy at birth\",\"Life Ladder\"]]\n",
    "x.plot.scatter(x=\"Healthy life expectancy at birth\", y=\"Life Ladder\",c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now the relationship between the predictor and response of interest doesn't appear to be linear. \n",
    "\n",
    "We now intend to expand the feature space to include higher order terms using, as basis function, a polynomial function of degree 2.\n",
    "\n",
    "You can notice, below, a pipeline. To gain some insight on what is the meaning of a pipeline, please check the PDSH book, [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html) (feature pipelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Fill the missing lines below to fit the model to data and apply the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select model and import it\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 2. Select model hyperparameters (here, we will use a polynomial basis function, of degree 2)\n",
    "polymodel = make_pipeline(PolynomialFeatures(2), LinearRegression(fit_intercept=True))\n",
    "\n",
    "# 3. Arrange data in feature matrix (or vector if just 1 feature) and Target array\n",
    "x = happinessdataframe.dropna().loc(axis=0)[:,2016][[\"Healthy life expectancy at birth\",\"Life Ladder\"]]\n",
    "\n",
    "X = x[\"Healthy life expectancy at birth\"].values\n",
    "Y = x[\"Life Ladder\"].values\n",
    "\n",
    "# 4. Fit model to data\n",
    "# Your code here\n",
    "\n",
    "# 5. Apply model\n",
    "xfit = np.linspace(min(X), max(X), 100)\n",
    "# Your code here\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y, c='black')\n",
    "ax.set_xlabel(\"Healthy life expectancy at birth\")\n",
    "ax.set_ylabel(\"Life Ladder\")\n",
    "ax.plot(xfit, yfit, c='red');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we use Linear or Polynomial regression to model the relationship between Life expectancy and Happiness? \n",
    "\n",
    "Let us use historic data (2016) to fit a model and use it to predict Happiness (Life Ladder) in 2017. We'll use a Linear or Polynomial regression. To quantify which model predicts better the relationship between Life expectancy and Happiness in 2017 we will compute the mean square error (MSE) and the coefficient of determination (R2) score between the real data in 2017 and our prediction.\n",
    "\n",
    "Notice that now we are considering a training (2016 data) and testing set (2017 data) â€” when doing hyperparameter tuning it is also common to use a validation set (more details below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE and R2\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "\n",
    "# 2016 will be our TRAIN set\n",
    "x = happinessdataframe.dropna().loc(axis=0)[:,2016][[\"Healthy life expectancy at birth\", \"Life Ladder\"]]\n",
    "\n",
    "Xtrain = x[\"Healthy life expectancy at birth\"].values\n",
    "Ytrain = x[\"Life Ladder\"].values\n",
    "\n",
    "polymodel = make_pipeline(PolynomialFeatures(1), LinearRegression(fit_intercept=True))\n",
    "polymodel.fit(Xtrain[:, np.newaxis], Ytrain)\n",
    "\n",
    "# 2017 will be our TEST set\n",
    "x = happinessdataframe.dropna().loc(axis=0)[:,2017][[\"Healthy life expectancy at birth\",\"Life Ladder\"]]\n",
    "\n",
    "Xtest = x[\"Healthy life expectancy at birth\"].values\n",
    "Ytest = x[\"Life Ladder\"].values\n",
    "\n",
    "YPredictionTest = polymodel.predict(Xtest[:, np.newaxis])\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error - Test: %.2f\" % mean_squared_error(Ytest, YPredictionTest))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score - Test: %.2f' % r2_score(Ytest, YPredictionTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "The previous evaluation metrics refer to a Linear regression (note the *PolynomialFeatures(1)*). What are the mean squared error and coefficient of determination (R2) if we apply a polynomial regression of degree 2? Does that mean that we should opt for a linear or polynomial regression, to model the relationship between Life expectancy and Happiness?**\n",
    "\n",
    "*Expected output:* \n",
    "\n",
    "MSE and R2 (aka coefficient of determination or variance score) associated with the polynomial regression with degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "We could use data before 2016 to train our model. Will that improve test error and variance score? Check that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "1) Create a regression model to capture the relationship between 'Freedom to make life choices' and 'Perception of corruption' in 2017. 2) Do a scatter plot with the corresponding data and regression line. Which order of basis function better capture the relationship between Freedom to make life choices and Perception of corruption in 2017?\n",
    "\n",
    "*Expected output:*\n",
    "\n",
    "You can check visually which order better captures the relationship in data (try order 1 to, say, 5); alternativelly, calculate the MSE and R2 associated with each order.\n",
    "\n",
    "*Recall that you change the order of the polynomial basis function by changing argument X in PolynomialFeatures(X)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Why do you think we need a test and validation set? Why not using just a single set to do validation and tests?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Classification (Naive Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we applied Regression methods given a structured dataset. We will now apply classification methods to an unstructured (text) dataset. Notice that instead of having continuous targets we will have discrete categorical targets. \n",
    "\n",
    "We will use as example the (quite famous) [Newsgroup](http://qwone.com/~jason/20Newsgroups/) dataset. The [20 Newsgroups collection](http://qwone.com/~jason/20Newsgroups/) has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. This example will be based on the [Scikit documentation](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html). \n",
    "\n",
    "The main goals of the following section are to have you experiment your skills with unstructured data (text), play with feature engineering in text and apply a classifcation algorithm (Naive Bayes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import newsgroup dataset from sklearn datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# Select the categories of interest\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "# Sample a training set\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a feature vector from unstructured text, we will encode *text* as *numbers* by performing simple word counting. We will assign each word to a numerical *id* and count the occurrence of each word. We can easily perform such task by using the *CountVectorizer* class included in Scikit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above is a sparse matrix. It is efficient to represent the number of times each word appears as a sparse matrix as most of the entries in the matrix will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check the contents of the word matrix:\n",
    "pd.DataFrame(X_train_counts.toarray(), columns=count_vect.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now apply the (Multinomial) Naive Bayes; \n",
    "\n",
    "# Remember the 5 steps we need to apply...\n",
    "\n",
    "# 1. Select model and import it\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 2. Select model hyperparameters; alpha is a smoothing parameter\n",
    "model = MultinomialNB(alpha=10)\n",
    "\n",
    "# 3. Arrange data in feature matrix / perform feature engineering\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "Y = twenty_train.target\n",
    "\n",
    "# 4. Fit model to data\n",
    "clf = model.fit(X_train_counts, Y)\n",
    "\n",
    "# 5. Apply model to new examples\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast','Help with printer','My knee hurts']\n",
    "\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "\n",
    "predicted = clf.predict(X_new_counts)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a model trained on previous examples of text and categories and applied it to new example. \n",
    "\n",
    "In general, how *good* is our model? Here we will validate the model we created and understand how to tune it to perform better.\n",
    "\n",
    "We will use the **TfidfTransformer**: this will transform a count matrix to a normalized tf or tf-idf representation; *Tf* means term-frequency while *tf-idf* means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification. The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document (as in the previous example) is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus. If needed, more info [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB(alpha=1))])\n",
    "\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# create a test set -> this is a set different than the train set\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "\n",
    "#in out many examples if our model guessing the right post category?\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we are computing model accuracy, as the fraction of correctly predicted text classes.\n",
    "\n",
    "In general, we can distinguish between the number of examples belonging to each class that that are correctly or incorrectly classified; this leads to the so-called *confusion matrix*. Each row of the matrix represents the instances in an actual class (true value) while each column represents the instances in a predicted class (predicted value). The name stems from the fact that it makes it easy to see whether the system is confusing two classes (i.e. commonly mislabeling one as another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(twenty_test.target, predicted)\n",
    "\n",
    "print(mat)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(mat, cmap='viridis', interpolation='nearest')\n",
    "ax.set_xlabel('predicted value')\n",
    "ax.set_ylabel('true value');\n",
    "ax.set_xticks([0,1,2,3])\n",
    "ax.set_yticks([0,1,2,3])\n",
    "ax.set_xticklabels(categories, rotation='vertical')\n",
    "ax.set_yticklabels(categories)\n",
    "\n",
    "# Loop over data to create text annotations.\n",
    "for i in range(len(mat)):\n",
    "    for j in range(len(mat)):\n",
    "        text = ax.text(j, i, mat[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "\n",
    "- a) How many times are texts classified as med?\n",
    "\n",
    "- b) How many times are texts not classified as med?\n",
    "\n",
    "- c) How many times are texts belonging to category med correctly classified? (True Positives - TP)\n",
    "\n",
    "- d) How many times are texts belonging to category med wrongly classified? (False Negative - FN)\n",
    "\n",
    "- e) How many times are texts not belonging to category med correctly classified as not being med? (True Negatives - TN)\n",
    "\n",
    "- f) What is the sensitivity of this model associated with class med? (Sensitivity = TP/(TP+FN))\n",
    "\n",
    "- Q9.7: What is the specificity of this model associated with class christian? (Specificity = TN/(TN+FP))\n",
    "\n",
    "*Expected output:* \n",
    "\n",
    "One numerical value for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answers here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
